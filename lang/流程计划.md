
# 方案 A 落地流程计划（Gemini API Key + HuggingFace Embedding）

目标：按 `计划.md` 的**方案 A：标准 RAG 知识库系统（基础版）**实现两条链路：

- **索引（Indexing / 入库）**：Markdown -> 切分 -> 向量化 -> 存入 ChromaDB（离线跑一次/增量更新）
- **检索（Retrieval / 问答）**：问题 -> Top-k 检索 -> Prompt 拼接 -> Gemini 生成答案（在线每次提问都跑）

> 约束：LLM 使用 Gemini 的 API Key；Embedding 使用 HuggingFace 本地模型（不依赖 OpenAI embedding）。

---

## 第一阶段：环境与配置准备（Pre-check）

### 1) 目录与持久化规划

- 数据源目录：`data/`（或你现有的 `coze/` 目录）
- Chroma 持久化目录：例如 `database/chroma_db/`
  - 必须用**持久化模式**，不要用内存模式，否则关程序就丢库

### 2) 环境变量（Gemini）

- 建议使用 `.env` 或系统环境变量，避免写死 Key
- 统一变量名：`GEMINI_API_KEY`（或 `GOOGLE_API_KEY` 二选一）

### 3) Embedding（HuggingFace，本地）

你需要提前决定 1 个模型并固定下来（后续入库、检索必须同一 embedding）。

推荐选择（示例）：

- `BAAI/bge-m3`（通用、多语、检索效果好）
- 或其它你确认可用的 sentence-transformers 模型

准备检查点：

- 模型首次运行会下载到本地缓存（确保网络/镜像源/磁盘空间可用）
- 如果机器是 Mac，可考虑是否使用 `mps`；否则默认 `cpu`
- 记得固定 `normalize_embeddings` 等关键参数（避免“入库/检索不一致”）

---

## 第二阶段：索引流程（Indexing / 入库）

执行频率：**首次全量 1 次**；文档新增/更新时再跑。

### 步骤 1：读取 Markdown（Loader）

- 遍历目录：递归扫描 `data/`（或 `coze/`）下所有 `.md`
- 读取文本：把全文读入
- 记录元数据（强制做）：
  - `source`：文件相对路径（用于回溯答案来源）
  - 可选：`title`、`mtime`（后续做增量更新会用）

### 步骤 2：切分（Splitting）

优先用“按标题切分 + 再按长度兜底”的组合：

1) `MarkdownHeaderTextSplitter`
   - 配置标题层级（例如 `#` / `##` / `###`）
   - 让切分后的每个 chunk 自动带上所属标题到 `metadata`
2) （推荐）`RecursiveCharacterTextSplitter` 二次切分兜底
   - 用于解决“某一节太长”导致 chunk 过大
   - 建议 chunk 大小：`500~1000` 字符；重叠：`50~150`

### 步骤 3：向量化并写入 Chroma（Embedding & Store）

- 初始化 HuggingFace Embedding（本地模型）
- 初始化 Chroma（指定 `persist_directory=database/chroma_db`）
- 写入方式：
  - 首次：全量写入
  - 后续（可选）：按 `source` + `mtime/hash` 做增量（先不做也可以）

完成验收：

- 输出统计：共写入 chunks 数量
- 抽查：随机打印 1～2 个 chunk 的 `metadata['source']` 与标题信息是否正确

---

## 第三阶段：检索流程（Retrieval / 问答）

执行频率：**每次用户提问**都运行。

### 步骤 1：接收问题

输入：用户一句自然语言问题（例如“公司的报销流程是什么？”）。

### 步骤 2：向量检索（Top-k Search）

- 连接同一个 Chroma 持久化目录（只读也行）
- 使用同一个 HuggingFace Embedding（必须一致）
- 相似度检索：
  - `k=5` 起步（可调 3/5/8）
  - 返回 `Document` 列表（chunk + metadata）

### 步骤 3：组装上下文（Context Assembly）

- 提取检索到的 chunk 文本，拼成 `context_text`
- 建议同时拼上来源，方便定位：
  - `source`（文件名）
  - `header`（标题层级）

### 步骤 4：Prompt 模板（严格基于资料）

系统指令建议固定为：

- 你是一个智能知识库助手。请严格根据【背景资料】回答【问题】。
- 如果背景资料中没有答案，回复：`知识库中未找到相关信息。`
- 不要编造，不要扩展到背景资料之外。

### 步骤 5：Gemini 生成回答（Generation）

- 调用 Gemini Chat 模型（仅普通对话生成，不需要工具/函数调用）
- 返回最终答案给用户

---

## 代码产物规划（先写清楚再动手）

建议拆两段脚本（与方案 A 保持一致）：

- `ingest.py`：入库脚本（Loader -> Split -> Embed -> Chroma 持久化）
- `chat.py`：问答脚本（Load Chroma -> Retrieve -> Prompt -> Gemini -> 输出）

可选增强（后面再做，不要一开始就加复杂度）：

- 增量更新：按文件 hash/mtime 判断是否重建向量
- 引用溯源：在回答末尾输出“来源文件/章节”
- 基础评测：固定 10 个问题做命中率检查
